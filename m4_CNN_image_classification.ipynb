{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6c0af8",
   "metadata": {},
   "source": [
    "# CNN for Image Classification\n",
    "### In this notebook we creat a CNN from ground up, by classifying images from images downloaded for different monkey species.\n",
    "\n",
    "### Following are the steps are included in image classification using CNN:\n",
    "\n",
    "* 1. Downloading the Dataset and specifying the **training config()**\n",
    "* 2. Setup the Pre-Training Processing i.e., **Resizing**,**Normalization** etc.\n",
    "* 3. Making/Modifying new images within the training dataset.\n",
    "* 4. Defining the **CNN** architecture [Printing the summary with number of parameters]\n",
    "* 5. Model training and Evaluation\n",
    "* 6. Saving and Loading the best model\n",
    "* 7. Inference \n",
    "* 8. Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5ef90",
   "metadata": {},
   "source": [
    "<center><img src='https://www.dropbox.com/scl/fi/e4541jejdlxzny3vrgw24/Monkey_architecture-updated.png?rlkey=4c3jm0kgzwm4txn1mbewqbox9&st=vq1lb2nt&dl=1' ></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36952664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchinfo import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torchvision.ops import Conv2dNormActivation\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a180097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed for reproduceability\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "set_seed(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f3658",
   "metadata": {},
   "source": [
    "### 1. Download and Extract the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be616ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L \"https://www.dropbox.com/s/45jdd8padeyjq6t/10_Monkey_Species.zip?dl=1\" -o \"10_Monkey_Species.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b92f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q \"10_Monkey_Species.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf01e73d",
   "metadata": {},
   "source": [
    "### 2. Dataset and Training Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd604bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "# Setting up the training config, so that we can make changes in one places\n",
    "class TrainingConfig:\n",
    "    ''' Configuration for Training'''\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 40\n",
    "    learning_rate: float = 1e-4\n",
    "    \n",
    "    log_interval: int = 1\n",
    "    test_interval: int = 1\n",
    "    data_root: int = \"./\"\n",
    "    device: str = \"cuda\"\n",
    "    num_workers: int = 5\n",
    "    \n",
    "train_config = TrainingConfig()\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Available Device: {DEVICE}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38580131",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = os.path.join(train_config.data_root,\"10_Monkey_Species\",\"training\",\"training\")\n",
    "val_root = os.path.join(train_config.data_root,\"10_Monkey_Species\",\"validation\",\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"10_Monkey_Species\",\"monkey_labels.txt\"),sep=\",\",header=None)\n",
    "df.columns = [\"Label\",\"Latin Name\",\"Common Name\",\"Train Images\", \"Validation Images\"]\n",
    "df['Latin Name'] = df['Latin Name'].str.replace(\"\\t\", \" \")\n",
    "df[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b11fda",
   "metadata": {},
   "source": [
    "### 3. Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ce0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to get the Mean and Standard Deviation of the images.\n",
    "# We first write a function to get mean and std which takes dataloader and image_size as an input argument\n",
    "\n",
    "def get_mean_std(train_loader,img_size=(224,224),num_workers=2):\n",
    "    \n",
    "    batch_mean = torch.zeros(3)\n",
    "    batch_mean_sqrd = torch.zeros(3)\n",
    "    \n",
    "    for batch_data,_ in train_loader:\n",
    "        batch_mean += batch_data.mean(dim=(0,2,3)) # Pytorch dimensions = [B,C,H,W] = [0,1,2,3]\n",
    "        batch_mean_sqrd += (batch_data ** 2).mean(dim=(0,2,3))\n",
    "        #  the mean is calculated over the batch, height, and width dimensions. \n",
    "        # The result will have a shape of (Channels,). \n",
    "        # This means you'll get a single mean value for each channel.\n",
    "        \n",
    "    mean = batch_mean / len(train_loader)\n",
    "    \n",
    "    #Variance is the mean of the squares minus the square of the mean.\n",
    "    var = (batch_mean_sqrd/len(train_loader)) - (mean ** 2)\n",
    "    \n",
    "    std = var ** 0.5\n",
    "    \n",
    "    print(f\"Mean : {mean}, Std : {std}\")\n",
    "    return mean,std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the transformations and pre-processing required for the Training and Validation Dataset\n",
    "img_size = (224,224)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(img_size,antialias=True),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4aabd",
   "metadata": {},
   "source": [
    "PyTorch has inbuilt functionality `(torchvision.datasets.ImageFolder class)` to load such structured image folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8005d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=<function default_loader>, is_valid_file=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887817b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data with transformation i.e, converting the required size and Totensor\n",
    "train_data_mean_calc = datasets.ImageFolder(root=train_root,transform=preprocess) \n",
    "# Loading the Train_loader to get the mean and std for the dataset\n",
    "train_loader_mean_calc = DataLoader(train_data_mean_calc,batch_size=train_config.batch_size,shuffle=True,num_workers=train_config.num_workers)\n",
    "\n",
    "mean,std = get_mean_std(train_loader_mean_calc)\n",
    "\n",
    "print(f\"Dataset Mean : {mean}\")\n",
    "print(f\"Dataset Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6225327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pre-processing for Training DataSet\n",
    "train_transform = transforms.Compose([\n",
    "    preprocess,\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomErasing(p=0.4),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomAffine(degrees=(30,70),translate=(0.1,0.3),scale=(0.5,0.75))\n",
    "    ], p=0.1), # This line will apply rotation, translation and Scaling 10% of the time for the given range\n",
    "    transforms.Normalize(mean=mean,std=std)\n",
    "])\n",
    "\n",
    "# Pre-processing for Common and Validation Data Set\n",
    "common_transform = transforms.Compose([\n",
    "    preprocess,\n",
    "    transforms.Normalize(mean=mean,std=std)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32cbbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply augmentations to the training dataset\n",
    "train_data = datasets.ImageFolder(root = train_root, transform = train_transform)\n",
    "\n",
    "# The validation dataset should have only common transforms like Resize, ToTensor and Normalize.\n",
    "val_data = datasets.ImageFolder(root=val_root, transform = common_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b62d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got the data, now we call the loader\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    shuffle = True,\n",
    "    batch_size = train_config.batch_size,\n",
    "    num_workers = train_config.num_workers\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    shuffle = False,\n",
    "    batch_size = train_config.batch_size,\n",
    "    num_workers = train_config.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48735616",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ace58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "\n",
    "    0: \"mantled_howler\",\n",
    "    1: \"patas_monkey\",\n",
    "    2: \"bald_uakari\",\n",
    "    3: \"japanese_macaque\",\n",
    "    4: \"pygmy_marmoset\",\n",
    "    5: \"white_headed_capuchin\",\n",
    "    6: \"silvery_marmoset\",\n",
    "    7: \"common_squirrel_monkey\",\n",
    "    8: \"black_headed_night_monkey\",\n",
    "    9: \"nilgiri_langur\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the image\n",
    "def visualize_images(dataloader, num_images = 20):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    #Iterate over the first batch\n",
    "    images, labels = next(iter(dataloader))\n",
    "    # print(images.shape)\n",
    "\n",
    "    num_rows = 4\n",
    "    num_cols = int(np.ceil((num_images / num_rows)))\n",
    "\n",
    "    for idx in range(min(num_images, len(images))):\n",
    "        image, label = images[idx], labels[idx]\n",
    "\n",
    "\n",
    "        ax = fig.add_subplot(num_rows, num_cols, idx+1, xticks = [], yticks = [])\n",
    "\n",
    "        image = image.permute(1,2,0)\n",
    "\n",
    "        #Normalize the image to [0,1] to display\n",
    "\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "        ax.imshow(image, cmap=\"gray\")  # remove the batch dimension\n",
    "        ax.set_title(f\"{label.item()}: {class_mapping[label.item()]}\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_images(train_loader, num_images = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049cd86",
   "metadata": {},
   "source": [
    "### 4. CNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08295b6b",
   "metadata": {},
   "source": [
    "The very first convolutional layer in the first convolutional block. To define a convolutional layer in PyTorch, we call the nn.Conv2D() function, which accepts several input arguments. First, we define the layer to have 32 filters. The kernel size for first filter is 5 and the subsequent layers filter is 3 (which is interpreted as 3x3). We can use a padding option called same, which will pad the input tensor so that the output of the convolution operation has the same spatial size as the input. This is not required, but it’s commonly used. if you don’t explicitly specify this padding option, then the default behavior has no padding, and therefore, the spatial size of output from the convolutional layer will be slightly smaller than the input size. After each convolutional layer, we add a BatchNorm2d layer, which normalizes the activations of the previous layer at each batch, thereby improving the training speed and stability of the network. We use a ReLU activation function in all the layers in the Network except for the output layer. This sequence of Conv2d followed by BatchNorm2d and ReLU is called Conv2dNormActivation and torchvision has a convenience function to implement this by torchvision.ops.Conv2dNormActivation().\n",
    "\n",
    "There is also another alternative approach to specify Conv2d layers with nn.LazyConv2d() where the input_channels is automatically inferred from the previous Conv2d layers output_channels.\n",
    "\n",
    "The first two convolutional layers has 32 filters each, and then we follow that with a max pooling layer that has a window size of (2x2), so the output shape from this first convolution block is (218 x 218 x 32). Next, we have the second convolutional block, has 64 and 128 filters in each convolutional layer instead of 32, and then finally, the third and fourth convolutional block has 256 and 512 filters.\n",
    "\n",
    "Note: The number of filters in each convolutional layer is something that you will need to experiment with. A larger number of filters allows the model to have a greater learning capacity, but this also needs to be balanced with the amount of data available to train the model. Adding too many filters (or layers) can lead to overfitting, one of the most common issues encountered when training models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590c28a",
   "metadata": {},
   "source": [
    "The final layer in the feature extractor is the nn.AdaptiveAvgPool2d(), which applies a 2D adaptive average pooling over an input image composed of several input channels. This layer ensures that the output has a fixed size of H x W, regardless of the input size. The number of output features is equal to the number of input channels. This is particularly useful for making the subsequent fully connected layers agnostic to the input size.\n",
    "\n",
    "Before we define the fully connected layers for the classifier, we need to first flatten the two-dimensional activation maps that are produced by the last convolutional layer (which have a spatial shape of 3x3 with 512 channels). This is accomplished by calling the nn.Flatten() function to create a 1-dimensional vector of length 4608. We then add a densely connected layer with 256 neurons and a fully connected output layer with 10 neurons because we have ten classes in our dataset.\n",
    "\n",
    "We will show the different ways in which a Conv2d -> BatchNorm -> ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99053f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._model = nn.Sequential(\n",
    "            \n",
    "            # Conv2D Norm Block 1:\n",
    "            nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Conv2D Norm Block 2: Using LazyConv2D\n",
    "            nn.LazyConv2d(out_channels=64,kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.LazyConv2d(out_channels=128,kernel_size=3),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Conv2d Norm Activation Block 3: Using Conv2dNormActivation\n",
    "            Conv2dNormActivation(in_channels=128,out_channels=256,kernel_size=3),\n",
    "            \n",
    "            Conv2dNormActivation(in_channels=256,out_channels=256,kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            Conv2dNormActivation(in_channels=256,out_channels=512,kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Feed Forward Layers\n",
    "            nn.AdaptiveAvgPool2d(output_size=(3,3)),\n",
    "            \n",
    "            # Flatten\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            # Classification Head\n",
    "            nn.Linear(in_features=512*3*3,out_features=256),\n",
    "            nn.Linear(in_features=256,out_features=10)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self._model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04dc078",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "\n",
    "# Define the Optimizer\n",
    "optimizer = Adam(params=model.parameters(),lr=train_config.learning_rate)\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dummy_input = (1,3,244,244)\n",
    "\n",
    "# Log\n",
    "log_dir = \"runs/80epochs-3.3M_param_dropout\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "print(summary(model,dummy_input,row_settings=[\"var_names\"],device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0b880",
   "metadata": {},
   "source": [
    "### 5. Training and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model,train_loader):\n",
    "    model.train()\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # Log Losses\n",
    "    running_losses = 0\n",
    "    correct_predictions = 0\n",
    "    total_train_samples = 0\n",
    "    \n",
    "    for images,labels in tqdm(train_loader,desc=\"Training\"):\n",
    "        images, labels = images.to(DEVICE),labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = F.cross_entropy(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_losses += loss.item()\n",
    "        _,predicted = torch.max(outputs.data,dim=1)\n",
    "        total_train_samples += labels.shape[0]\n",
    "        correct_predictions += (predicted==labels).sum().item()\n",
    "        \n",
    "    train_avg_loss = running_losses / len(train_loader)\n",
    "    training_accuracy = 100 * correct_predictions/total_train_samples\n",
    "    \n",
    "    return train_avg_loss, training_accuracy        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98247ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(val_data,val_loader):\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    running_loss = 0\n",
    "    total_val_samples = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for images,labels in tqdm(val_data,desc=\"Validation\"):\n",
    "        images,labels = images.to(DEVICE),labels.to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            \n",
    "        loss = F.cross_entropy(outputs)\n",
    "        running_loss += loss.item()\n",
    "        total_val_samples = labels.shape[0]\n",
    "        _,predicted = torch.max(outputs.data,dim=1)\n",
    "        correct_predictions += (predicted==labels).sum().item()\n",
    "        \n",
    "    validation_avg_loss = running_loss / len(val_loader)\n",
    "    validation_accuracy = 100 * correct_predictions / total_val_samples\n",
    "    \n",
    "    return validation_avg_loss,validation_accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcefc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model,train_loader,val_loader):\n",
    "    train_losses, train_accuracies = [],[]\n",
    "    val_losses,val_accuracies = [],[]\n",
    "    \n",
    "    best_validation_acc = 0.0\n",
    "    best_weights = None\n",
    "    \n",
    "    for epoch in tqdm(range(train_config.num_epochs)):\n",
    "        train_loss,train_accuracy = training(model,train_loader)\n",
    "        val_loss,val_accuracy = validation(model,val_loader)\n",
    "        \n",
    "        train_losses.appedn(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:0>2}/{train_config.num_epochs} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% - Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        writer.add_scalar('Loss/Train',train_loss)\n",
    "        writer.add_scalar('Loss/Val',val_loss)\n",
    "        writer.add_scalar('Accuracy/Train',train_accuracy)\n",
    "        writer.add_scalar('Accuracy/Val',val_accuracy)\n",
    "        \n",
    "        if val_accuracy > best_validation_acc:\n",
    "            best_validation_acc = val_accuracy\n",
    "            best_weights = model.state_dict()\n",
    "            print(f\"Saving Best Model.......\")\n",
    "            torch.save(best_weights,\"best.pt\")\n",
    "        \n",
    "    \n",
    "    return train_losses,train_accuracies,val_losses,val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44fae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies, val_losses, val_accuracies = main(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56592ec",
   "metadata": {},
   "source": [
    "### 6. Saving and Loading the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load(\"best.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67d56bd",
   "metadata": {},
   "source": [
    "### 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dbd8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model,val_loader):\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    all_images,all_labels = [],[]\n",
    "    all_pred_indices,all_pred_probs = [],[]\n",
    "    \n",
    "    for images,labels in val_loader:\n",
    "        images,labels = images.to(DEVICE),labels.to(DEVICE)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = model(images)\n",
    "            \n",
    "        probs = F.softmax(outputs,dim=1)\n",
    "        pred_indices = probs.data.max(dim=1)[1]\n",
    "        pred_prob = probs.max.data(dim=1)[0]\n",
    "        \n",
    "        all_images.append(images.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "        all_pred_indices.append(pred_indices.cpu())\n",
    "        all_pred_probs.append(pred_prob.cpu())\n",
    "        \n",
    "    return (torch.cat(all_images).numpy(),torch.cat(all_labels).numpy(),torch.cat(all_pred_indices).numpy(),torch.cat(all_pred_probs).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe4c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-normalize image\n",
    "def denormalize(image):\n",
    "    mean_ar = np.array(mean)\n",
    "    std_ar = np.array(std)\n",
    "    image = image * std_ar + mean_ar\n",
    "    \n",
    "    return np.clip(image,0,1) # np.clip makes the pixel values to be in range of [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_predictions(sample_images,sample_gt_labels, pred_indices, pred_probs, num_images =10):\n",
    "\n",
    "    fig = plt.figure(figsize = (20,5))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        idx = random.randint(0, len(sample_images) -1)\n",
    "        image = sample_images[idx].transpose(1,2,0) #(C,H,W) --> (H,W,C)\n",
    "        label = sample_gt_labels[idx]\n",
    "        pred_idx = pred_indices[idx]\n",
    "        pred_prob = pred_probs[idx]\n",
    "\n",
    "        image = denormalize(image)\n",
    "\n",
    "        ax = fig.add_subplot(1, num_images, i+1)\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(f\"GT: {class_mapping[label]}\\nPred: {class_mapping[pred_idx]} ({pred_prob:.2f})\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images, val_gt_labels, pred_indices, pred_probs = prediction(model, val_loader)\n",
    "\n",
    "visualise_predictions(val_images, val_gt_labels, pred_indices, pred_probs, num_images = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
